<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>All Data is Local</title>
    <link rel="stylesheet" href="../main.css">
    <script src="https://code.jquery.com/jquery-1.12.4.js"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-J2SEN9LXNV"></script>
    <script src="https://d3js.org/d3.v6.min.js"></script>

</head>
<body data-project-name="Mapping Religious Persecution in Rio de Janeiro"> <!-- The name here is to link Tags from json file (project name)-->
    <div class="top-bar">
   
        <div class="title">
            <a href="../index.html" id="sidePanel">
                Methods and Tutorials in Spatial Research 
            </a>
        </div>

        <div class="button-container" id="about">
            <a href="../about.html">About</a>
        </div>
     </div>

    <div class="column">

        <div class="column-left">
            <div id="tutorials-side-panel">
                <h6>
                    Currently viewing tutorial page, <br><a href="../index.html">click here to return to main page.</a>
                </h6>

                <h3>Workshop Panel: </h3>
                <div class="visual-code-container">
                    <div class="tutorials-circle"></div>
                    <p id="panel-text"></p>
                </div>
                <h3>Project Tags: </h3>
                <div class="tutorials-ProjectTagsContainer" id="project_tags">
                    <!-- Tags will be dynamically inserted here -->
                </div>
            </div>
        </div>
        <div class="column-main">
            <div class="tutorials-section">
                <h1>Mapping Religious Persecution in Rio de Janeiro</h1>
                <h2>
                    Ana Paulina Lee | Associate Professor of Luso-Brazilian Studies <br> 
                    Jared Lieberman | MS Statistics and Data Scientist <br>
                    Research Assistants: Jacqueline Grac McMahon Vera and Ashley Elizabeth Young
                </h2>

                <p>
                    Jump to: <br>
                    <a href="#Introduction">Introduction</a><br>
                    <a href="#Tutorial"> Technical Tutorial</a> <br>
                    <a href="#part1" class="indent">Part 1:  Collecting data from newspapers</a><br>
                    <a href="#part2" class="indent">Part 2:  Text analysis, <span style="font-style: italic;">More than a Map Visualization</span></a>
                </p>

                <h3 id="Introduction">Introduction</h3>

                    <p>This tutorial provides a methodology for examining the role that print media played in animating Rio de Janeiro’s urban imagination. For this study, I collaborated with data scientist Jared Lieberman to create a database of terms related to Afro-Brazilian religions such as candomblé and umbanda with the aim of tracking the role that newspapers played in depicting African diasporic religious and cultural practices in the vocabulary of demonology. To do this research, I created a “sorcery glossary” that included terms as <i>feitiçaria</i> (sorcery) and <i>magia negra</i> (black magic) to examine how journalists translated Afro-diasporic religious, cultural, and linguistic terms such as <i>candomblé, umbanda, mandinga, and macumba</i> into Catholic and Portuguese semantic and symbolic worlds associated with demonology.</p>

                    <p>
                        Concerns with devilry, superstitious practices, and demonic magic are part of a longer theological and intellectual history of Catholic thought. The Portuguese Catholic empire spread across the globe in different yet connect ways. A primary characteristic of the Portuguese imperial experience included the spread of Catholicism as a means of conversion and colonization through translating local practices, kinship structures, and trade relations into the symbolic and semantic structures of Catholicism and Portuguese. In Brazil, in the context of colonial extraction and slavery, missionaries and colonialists turned their attention to converting Indigenous people and enslaved Africans, and treated their engagements with non-Catholic spiritual and healing practices as actionable offenses, punishable with banishment, enslavement, and death. Brazilian independence from Portugal, and national independence, did not put an end to these histories of religious persecution. Rather, Brazilian republican law adapted the language of religious intolerance to national concerns related to presenting Brazil on the world stage as a modern, Christian nation with a racial whitening agenda.
                    </p>

                    <p>
                        Print media played a critical part in producing the urban geographical imagination. <i>The Gazeta de Notícias</i> and the <i>Jornal do Brasil</i>, two of Brazil’s most widely circulated and read newspapers, printed and distributed thousands of stories that associated Afro-Brazilian religions with demonic magic. The <i>Jornal do Brasil</i> was founded in 1891 and remains in circulation. Between 1891-2010, the <i>Jornal do Brasil</i> published 5054 stories on feiticeiros (sorcerers) and 4487 reports on candomblé. Between 1896-2010, it published 9494 stories on macumba. The <i>Gazeta de Notícias</i> circulated from 1875 to 1956. During those years, it published 1592 stories on feiticeiros and 97 stories on candomblé. Between 1881 to 1956, it published 565 articles on macumba.
                    </p>
                    <p>
                        Journalists contributed to inventing an urban social imagination about Afro-Brazilian religions by mapping a relation between African Atlantic spiritual practices and cultural gatherings with marginality, poverty, and danger.  Newspapers listed addresses and names of people, and journalists accompanied the police in raiding homes and temples. Their stories steadily reinforced a relationship between <i>feitiçaria</i> (sorcery) and <i>magia negra</i> (black magic) with Afro-Brazilian religions and cultures, and their depictions criminalized candomblé and umbanda priests, priestess, and religious and cultural gatherings as noisy, disruptive, and morally dangerous. Tales about the black magic activities happening in various parts of the city cast a spatial understanding onto these neighborhoods, and shed light on the cultural role that newspapers played in mapping affective geographies onto the urban landscape. Print media played a central role in demonizing African Atlantic religions like candomblé and umbanda to enforce discourses of Catholic supremacy as an issue of public health. Journalists named priests and priestesses and listed the addresses of their terreiros (African diasporic sacred spaces); hence, their chronicles produced spatial and affective understandings about urban economic geographies that paved the way to blighting neighborhoods as centers of supernaturally bad activities, and thus fit for urban renewal projects. These stories cultivated the cultural and moral grounds to justify the destruction or neglect of areas known as centers of African Brazilian cultural, religious, and social life. 
                    </p>
                    <p>
                        To give an example of the typical rhetoric used in these stories, below are excerpts from an article published in the <i>Gazeta de Notícias</i> on May 5, 1918 titled, “A Macumba Interrupted: 25 Imprisoned” and subtitle, “The <i>apetrechos</i> (instruments, tools) used in “candomblé” and those arrested by the police of the 20th district.”
                    </p>
                    <p class="indent" style="text-indent:0px">
                        A few days ago, the delegate of the 20th district, Dr. Gualberto de Oliveira Filho, received a report that Arsenio Vieira de Magalhães and his wife, “Marinquinhas Chucaleira,” were performing “macumba” in house no. 33 on Gaspar Street in Terra Nova. The noise of the feiticeiros (sorcerer, religious healers) interrupted the sleep of the neighbors, who were already surprised by the inaction of the police. Sending after carrying out a rigorous investigation, the police delegate came to the conclusion that Arsenio and his wife were the leaders of the “macumba,” in which many women and children took part. To put an end to the exploitation, the police delegate ordered the commissar Dr. Morães to raid the house and arrest all the “macumbeiros.” At 3 a.m. yesterday morning, in compliance with the order, Dr. Morães proceeded to the place indicated and, after surrounding the house, entered it, followed by soldiers. The dancing was very lively, and the sorcerers, upon seeing the police, began to disperse. The following people were arrested, in addition to the owners of the house: Theodoro Gonçalves da Costa, an ex-police soldier, who lives in the Saude barracks, João Silva, who lives in Rua da Saude. Francisco Luiz n. 87, Eustachio Vieira, resident at Rua Visconde de Sapucahy n. 113, Olympio Miranda, resident at Rua Augusto Nunes n. 69, Abelardo Ribeiro da Silva, resident at Rua Santa Philomena n. 34, casa 1; João Baptista da Rocha, resident at Rua Cunha Barbosa n. 77; Manoel da Silva Gaspar, resident at Rua Gaspar n. 33; Antonio Rodriguez, resident in the same house; José de Carvalho Silva, resident at Rua Dorothéa Eugenia n. 84; Antonio Ferreira Mendes, resident at Rua Visconde do Rio Branco n.47 Augusto Costa, resident at Rua Vista Alegre n. 16; Elvira Tavares, resident at Rua Francisco Ziezi n. 87; Guiomar Tavares, resident at Rua Senador Pompeu numero 203; Isabel de Mattos, resident at Rua Francisco Eugenio n. 125; Maria Lydia de Mello, resident at Rua Visconde de Sapucahy n. 113; Olivia dos Santos, resident at Rua Francisco Ziezi n. 86; Joaquina Rosa dos Santos, resident at Rua Páo Ferro n. 41; Joanna Evangelista, in the same house; Anna Evangelista, resident at Rua Francisco Ziezi n. 87. Sorcery paraphernalia such as figurines, daggers, magic sticks, cardboard fish, barrels of different colors, sticks, bows, swords, black hens, rosaries, saints, etc. were seized. They have all been locked up and will be prosecuted.
                    </p>

                    <h5>
                        *This research is an excerpt from my forthcoming book, Coding Witchcraft: Race, Religion, and Public Health. I would like to thank Laura Kurgan and Adeline Chum for inviting me to participate in the Center for Spatial Research workshops and for their feedback on this project. 
                    </h5>

                <h3 id="Tutorial">Technical Tutorial</h3>

                <h4 id="part1">Part 1:  Collecting data from newspapers</h4>
                <h4>Data</h4>
                    <p>
                        The data are retrieved from the Brazilian Digital Library can be accessed <a href="https://memoria.bn.gov.br/hdb/periodico.aspx">here</a>.
                    </p>
                    <p>The database includes a massive amount of articles broken down by newspaper and year of publication. This digital library has been particularly useful for our goals because of its ease of searching newspapers by individual terms. This has allowed us to quickly select specific articles in PDF format without having to search through entire newspapers. The digital library also provides certain metadata that we have integrated into our data collection process. In particular, we are able to gather the number of matches within a newspaper for terms over time.
                    </p>
                    <p>
                        An example of a search from the digital library homepage is shown below.
                    </p>

                    <div class="image">
                        <img src="./images/MappingReligiousPersecution/image2.png" alt="Search in Brazilian Digital Library">
                    </div>

                    <p>An example of the metadata for individual articles is found below.</p>
                    <div class="image">
                        <img src="./images/MappingReligiousPersecution/image3.png" alt="Search in Brazilian Digital Library">
                    </div>

                    <p>
                        The metadata were obtained from a mixed process of manually recording data and web scraping. The code that runs this automated process can be found on <a href="https://github.com/jared-lieberman/CSR-Tutorial/blob/main/scrape_metadata.py">the GitHub repository linked here</a>. 
                    </p>

                    <p>
                        The algorithm uses two files: 
                    </p>
                    <p class="indent-num">
                        1. a main script</p>
                    <p class="indent-next">
                        2. a configuration file
                    </p>
                    <p>
                        The configuration file (config.py) defines several constants used in the scraping process, including the search terms (<span class="code-block">SEARCH_TERMS</span>), the output file name(<span class="code-block">OUTPUT_FILE</span>), and a dictionary mapping newspaper IDs to their corresponding names (<span class="code-block">NEWSPAPER_ID_DICT</span>). Additionally, the script allows the user to specify which newspaper to search within by setting the <span  class="code-block">NEWSPAPER_ID</span>.
                    </p>

                    <p>
                        The main script (scrape_metadata.py) uses Selenium to automate interactions with the website. It begins by initializing a Chrome web driver and navigating to the appropriate URL based on the selected newspaper's ID. The script includes functions to handle pop-ups, extract data from the web page, and hover over elements to reveal additional options. It iterates through the specified search terms, performing searches on the website and extracting relevant data for each term. The extracted data is then compiled into a Pandas DataFrame, which is saved as a CSV file. The entire process is automated to run seamlessly, with informative print statements throughout to track the script's progress and any issues encountered.
                    </p>

                    <p>
                        In order to run the code, the user must have a Jupyter environment running and the required packages installed. Additionally, the user needs to define the four variables in the configuration file: 
                    </p>
                    <p class="indent-num">
                        1. <span class="code-block">SEARCH_TERMS</span> (list),</p>
                    <p class="indent-next">
                        2. <span class="code-block">OUTPUT_FILE</span> (CSV string name),</p>
                    <p class="indent-next">
                        3. <span class="code-block">NEWSPAPER_ID_DICT</span> (to add more newspapers to search), and </p>
                    <p class="indent-next">
                        4. <span class="code-block">NEWSPAPER_ID</span> (string found from dict).
                    </p>
                    <p>
                    
                        The code can search through one newspaper at a time. Once these are set, then code in Jupyter can be run.
                    </p>

                    <p>
                        From these article ID strings, in combination with ID’s for individual newspapers, we were able to recreate the URLs of each article. 
                    </p>
                    <p>
                        For example, the <span class="code-block">ID</span> for Gazeta de Notícias is <span class="code-block">103730</span>, and a full form URL of an article in that newspaper is “https://memoria.bn.gov.br/pdf/103730/per103730_1875_00052.pdf”. Thus, the article that has an ID of <span class="code-block">00052</span> and was published in 1875. This process was conducted in Python. Once we pieced together the URLs, research assistants were able to more efficiently access relevant articles. 
                    </p>

                    <p>
                        As noted, the metadata also allowed us to conduct temporal analysis on the number of articles that included the searched term in a particular newspaper. This process, conducted in R, reformatted the data in order to prepare it for making relevant visualizations. <a href="https://github.com/jared-lieberman/CSR-Tutorial/blob/main/count_years.Rmd">Click here for theGitHub repository code</a>. 
                    </p>  
                    <p>Below is a graph of the number of article matches over time by term and newspaper. For this graph, we include nine terms across two newspapers.
                    </p>

                    <div class="image">
                        <img src="./images/MappingReligiousPersecution/image4.png" alt="Search in Brazilian Digital Library">
                    </div>  


                <h4 id="part2">Part 2:  Text analysis, <i>More than a Map Visualization</i></h4>

                <p>
                    The second part of the analysis encompasses a combination of text analysis, geolocation, and mapping in order to geospatially visualize the language of witchcraft and its policing over time. From the text of each article, we are working on an algorithm to conduct natural language processing (NLP) to extract locations of the incidents. The algorithm used for extracting address-like entities is Named Entity Recognition (NER). NER is a sub-task of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. For our analysis, we are particularly interested in location-related entities from the articles. 
                </p>

                <p>
                    The Jupyter Notebook geolocate.ipynb is designed to geolocate addresses from the dataset of newspapers texts and append the corresponding latitude and longitude values to the data. 
                </p>
                <p>
                    Here's a breakdown of what the code does and how to run it:
                </p>

                <p>
                    The notebook starts by importing the necessary libraries: 
                </p>
                <p class="indent-num">
                    1. pandas for data manipulation </p>
                <p class="indent-next">
                    2. geopy for geocoding. </p>
                <p class="indent-next">
                    3. Nominatim, a geocoding service provided by OpenStreetMap, is used for converting addresses into geographical coordinates (latitude and longitude). </p>
                <p class="indent-next">
                    4. A RateLimiter from geopy.extra is also set up to manage the rate at which geocoding requests are sent, preventing the service from being overwhelmed and to comply with usage limits.
                </p>
                <p>
                    The code then loads a CSV file (witchcraft in print media - Sheet1.csv) into a Pandas DataFrame and removes any rows that have missing values in the 'Address' column. This ensures that only valid addresses are passed to the geocoding function.
                </p>

                <p>
                    A function named get_lat_long is defined to take an address as input and return the corresponding latitude and longitude. This function appends "Rio de Janeiro, Brazil" to the address to improve geocoding accuracy, assuming that all addresses are within this region. The geocoding is done using the Nominatim service. If an address is successfully geocoded, the latitude and longitude are returned; otherwise, None is returned for both.
                </p>

                <p>
                    The get_lat_long function is applied to the 'Address' column of the DataFrame, and the results are stored in new columns for latitude and longitude. The DataFrame is then printed to the console to display the results. Finally, the updated DataFrame, now including latitude and longitude data, is saved to a new CSV file (Output_geolocate.csv). 
                </p>
                <p>
                    In order to run the code, ensure that you have the required Python libraries installed. You can install them using pip if they are not already available in your environment. With the inputted CSV file in the same directory as the notebook, open the Jupyter Notebook and run each cell sequentially. The notebook will load the data, geocode the addresses, and output the results to both the console and a new CSV file (Output_geolocate.csv). After running the notebook, you can check the output CSV file to see the geocoded data, which now includes latitude and longitude for each address.
                </p>
                <p>
                    The next step is to fill in missing data manually by looking at the Address column for rows without latitude and longitude values produced by the automated process. Copy all or a subset of the cells, and append “Rio de Janeiro, Brazil” to the text. Then, using a mapping service like Google Maps, search for the address and grab the latitude and longitude values. 
                </p>

                <p>
                    Once those values are filled in, the next step is to map the data. The Jupyter Notebook (<a href="https://github.com/jared-lieberman/CSR-Tutorial/blob/main/Map_texts.ipynb">Map_texts.ipynb</a>) is designed to visualize geographic data by plotting points on a map using latitude and longitude coordinates. The notebook begins by splitting a column named lat_long_manual into two separate columns: latitude and longitude. The lat_long_manual column appears to store latitude and longitude as a single string, separated by a comma (e.g., "40.7128, -74.0060"). These new columns are then converted to numeric data types for proper handling in later steps.
                </p>

                <p>
                    The notebook uses the folium library to create an interactive map. The map is centered around the average of the provided latitude and longitude coordinates, ensuring that the map is centered on the general region where the data points are located. The zoom level is set to 2, which provides a global view of the map but can be adjusted depending on the density and proximity of the data points. For each row in the DataFrame, a marker is added to the map at the corresponding latitude and longitude. These markers represent the geographic locations specified in the dataset, allowing for a visual representation of the data on the map. The created map is saved to an HTML file (map.html), which can be opened in any web browser for interactive exploration. Additionally, the map can be displayed directly within the Jupyter Notebook by simply outputting the folium.Map object. 
                </p>
                <div class="image">
                    <img src="./images/MappingReligiousPersecution/image5.png" alt="Search in Brazilian Digital Library">
                </div>
                <p>
                    From here, we can start to <a href="https://github.com/jared-lieberman/CSR-Tutorial/blob/main/Map_texts_macumba.ipynb">adjust the map</a> to analyze geospatial patterns with language used within the articles. For example, below is a map where the red points include the term “macumba” or “macumbeiro”.
                </p>

                <div class="image">
                    <img src="./images/MappingReligiousPersecution/image6.png" alt="Search in Brazilian Digital Library">
                </div>


        
            </div>

            
                
        </div>
        <div id="project_tags" class="tag-grid"></div>
    </div>

        <script>
            let tutorialsData;
            let colorsData;

            function loadData() {
                // Load both JSON files
                d3.json("../tutorials.json").then(function(loadedData) {
                    tutorialsData = loadedData;
                    d3.json("../colors.json").then(function(loadedColors) {
                        colorsData = loadedColors;
                        displayTagsAndPanel();
                    });
                });
            }

            function getColorForTag(tag) {
                // Find the panel color for a given tag
                const colorEntry = colorsData.colors.find(colorEntry => colorEntry.tags.includes(tag));
                return colorEntry ? colorEntry.color : "#CCCCCC"; // Default to grey if not found
            }

            function getPanelColor(panel) {
                // Find the color for a given panel
                const colorEntry = colorsData.colors.find(colorEntry => colorEntry.panel === panel);
                return colorEntry ? colorEntry.color : "#CCCCCC"; // Default to grey if not found
            }

            function displayTagsAndPanel() {
                const projectName = document.body.getAttribute("data-project-name");
                const tagContainer = document.getElementById("project_tags");
                const panelTextElement = document.getElementById("panel-text");
                const circleElement = document.querySelector(".tutorials-circle");

                const project = tutorialsData.tutorials.find(tutorial => tutorial.project === projectName);

                if (project) {
                    panelTextElement.innerText = project.panel;

                    // Find and set the color for the panel
                    const panelColor = getPanelColor(project.panel);
                    circleElement.style.backgroundColor = panelColor;

                    // Display and color the tags
                    project.tags.forEach(tag => {
                        const tagElement = document.createElement("div");
                        tagElement.className = "tag-item";
                        tagElement.innerText = tag;
                        tagElement.style.backgroundColor = getColorForTag(tag); // Use color for each tag
                        tagContainer.appendChild(tagElement);
                    });
                }
            }

            document.getElementById('sidePanel').addEventListener('click', function() {
                window.location.href = '../index.html'; 
            });

            loadData(); // Call the loadData function to load the JSON and display the tags and panel text
        </script>

</body>
</html>
