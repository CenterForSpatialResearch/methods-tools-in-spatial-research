<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mapping Afro-Brazilian Religious Histories: An AI-Assisted Tutorial</title>
    <link rel="stylesheet" href="../main.css">
    <script src="https://code.jquery.com/jquery-1.12.4.js"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-J2SEN9LXNV"></script>
    <script src="https://d3js.org/d3.v6.min.js"></script>

</head>
<body data-project-name="Mapping Afro-Brazilian Religious Histories: An AI-Assisted Tutorial"> <!-- The name here is to link Tags from json file (project name)-->
    <div class="top-bar">
   
        <div class="title">
            <a href="../index.html" id="sidePanel">
                Methods and Tutorials in Spatial Research 
            </a>
        </div>

        <div class="button-container" id="about">
            <a href="../about.html">About</a>
        </div>
     </div>

    <div class="column">

        <div class="column-left">
            <div id="tutorials-side-panel">
                <h6>
                    Currently viewing tutorial page, <br><a href="../index.html">click here to return to main page.</a>
                </h6>

                <h3>Workshop Panel: </h3>
                <div class="visual-code-container">
                    <div class="tutorials-circle"></div>
                    <p id="panel-text"></p>
                </div>
                <h3>Project Tags: </h3>
                <div class="tutorials-ProjectTagsContainer" id="project_tags">
                    <!-- Tags will be dynamically inserted here -->
                </div>
            </div>
        </div>
        <div class="column-main">
            <div class="tutorials-section">
                <h1>Mapping Afro-Brazilian Religious Histories: An AI-Assisted Tutorial</h1>
                <h2>
                    Ana Paulina Lee | Associate Professor of Luso-Brazilian Studies <br> 
                    Jared Lieberman | MS Statistics and Data Scientist <br>
                    Research Assistants: Jacqueline Grac McMahon Vera and Ashley Elizabeth Young
                </h2>

                <p>
                    Jump to: <br>
                    <a href="#Introduction">Introduction</a><br>
                    <a href="#Tutorial"> Technical Tutorial</a> <br>
                    <a href="#part1" class="indent">Part 1:  Collecting data from newspapers</a><br>
                    <a href="#part2" class="indent">Part 2:  Text analysis, <span style="font-style: italic;">More than a Map Visualization</span></a>
                </p>

                <h3 id="Introduction">Introduction</h3>

                    <p>Throughout the nineteenth and twentieth centuries, Rio de Janeiro's newspapers published thousands of stories that made “sorcery” (feitiçaria) and “black magic” (magia negra) synonymous with Atlantic African religions such as Candomblé and Umbanda. Journalists regularly provided names and addresses of the individuals they reported on, effectively mapping a range of religious practices into the city’s urban fabric. Through analyzing a glossary of about 60 words found in thousands of newspaper articles spanning two centuries, we focused on print media as a tool to examine the disciplinary logic of twentieth-century Rio de Janeiro during a presidency-turned-dictatorship of Getúlio Vargas (1930-1945).</p>

                    <p>
                        Print media outlets circulated stories and narratives that created affective architectures of urbanization—fears, hopes, and a sense of unease or opulence clustered around specific neighborhoods, streets, and addresses. Using geolocation, Natural Language Processing (NLP), computational linguistics, and manual searches, our team employed data science to spatialize newspaper stories about centers of Black Atlantic religious and cultural practices. These spatial layers of meaning reveal intersections between African diasporic worship and cultural spaces and Brazilian state efforts to organize cosmological and ecological environments into property and urban infrastructure, effectively mapping the overlapping religious, political, and cultural networks that constitute the geographical imagination of Rio de Janeiro.
                    </p>

                    <p>
                        Collaborating with Jared Lieberman, a Data Scientist, Rosie Robot, an AI-driven assistant, and research assistants Jacqueline Grac McMahon Vera and Ashley Elizabeth Young, we used a combination of human and computational realms to deepen our understanding of how language and space interact in these narratives. To prepare data for Named Entity Recognition (NER) and NLP, we parsed historical texts, identified key entities, and linked them to spatial and cultural contexts. Using newspapers to study affective urban geographies—sites of thrills, danger, and mystery, this tutorial provides a method for studying sentiment by geography to better understand the relationship between language and the material spaces of the city in crafting the economic and emotional experiences of urbanism.
                    </p>


                    <h3>
                        Analyzing Context and Sentiment
                    </h3>

                    <p>
                        Through sentiment analysis, Rosie Robot examined the tone of articles, uncovering biases in how journalists portrayed Afro-Brazilian religions. Articles often depicted religious practices and ceremonial gatherings as noisy, dangerous, or immoral, reinforcing stereotypes. For instance, phrases such as “the noise of the feiticeiros interrupted the sleep of the neighbors” conveyed disapproval, linking these rituals to disruption and criminality.
                    </p>

                    <p>
                        Additionally, Rosie Robot identified co-occurring terms, revealing thematic connections. Words like macumba often appeared alongside raid, crime, and noise, deepening the association between African diasporic spiritual practices and urban disorder.
                    </p>

                    <h3>
                        Interpretation and Implications
                    </h3>

                    <p>
                        Journalists did not merely report on Afro-Brazilian religions, but they actively constructed an affective geography that criminalized these practices and justified their suppression. An article published in the <i>Gazeta de Notícias</i> on May 5, 1918 titled “A Macumba Interrupted: 25 Imprisoned” and subtitle, “The <i>Apetrechos</i> (instruments, tools) used in Candomblé and Those Arrested by the Police of the 20th District” listed entities that can be mapped onto the cityscape. The article named individuals and addresses and provided details about police districts as well.
                    </p>

                    <p>
                        These narratives reinforced a new era of Church-State collaboration and also supported state efforts to urbanize and sanitize certain areas. By framing neighborhoods as centers of danger and immorality, these stories laid the groundwork for urban renewal projects that displaced African descendants, immigrants, and women from their homes and centers of work and worship.
                    </p>
                    <p class="indent" style="text-indent:0px">
                        A few days ago, the delegate of the 20th district, Dr. Gualberto de Oliveira Filho, received a report that Arsenio Vieira de Magalhães and his wife, “Marinquinhas Chucaleira,” were performing “macumba” in house no. 33 on Gaspar Street in Terra Nova. The noise of the feiticeiros (sorcerer, religious healers) interrupted the sleep of the neighbors, who were already surprised by the inaction of the police. Sending after carrying out a rigorous investigation, the police delegate came to the conclusion that Arsenio and his wife were the leaders of the “macumba,” in which many women and children took part. To put an end to the exploitation, the police delegate ordered the commissar Dr. Morães to raid the house and arrest all the “macumbeiros.” At 3 a.m. yesterday morning, in compliance with the order, Dr. Morães proceeded to the place indicated and, after surrounding the house, entered it, followed by soldiers. The dancing was very lively, and the sorcerers, upon seeing the police, began to disperse. The following people were arrested, in addition to the owners of the house: Theodoro Gonçalves da Costa, an ex-police soldier, who lives in the Saude barracks, João Silva, who lives in Rua da Saude. Francisco Luiz n. 87, Eustachio Vieira, resident at Rua Visconde de Sapucahy n. 113, Olympio Miranda, resident at Rua Augusto Nunes n. 69, Abelardo Ribeiro da Silva, resident at Rua Santa Philomena n. 34, casa 1; João Baptista da Rocha, resident at Rua Cunha Barbosa n. 77; Manoel da Silva Gaspar, resident at Rua Gaspar n. 33; Antonio Rodriguez, resident in the same house; José de Carvalho Silva, resident at Rua Dorothéa Eugenia n. 84; Antonio Ferreira Mendes, resident at Rua Visconde do Rio Branco n.47 Augusto Costa, resident at Rua Vista Alegre n. 16; Elvira Tavares, resident at Rua Francisco Ziezi n. 87; Guiomar Tavares, resident at Rua Senador Pompeu numero 203; Isabel de Mattos, resident at Rua Francisco Eugenio n. 125; Maria Lydia de Mello, resident at Rua Visconde de Sapucahy n. 113; Olivia dos Santos, resident at Rua Francisco Ziezi n. 86; Joaquina Rosa dos Santos, resident at Rua Páo Ferro n. 41; Joanna Evangelista, in the same house; Anna Evangelista, resident at Rua Francisco Ziezi n. 87. Sorcery paraphernalia such as figurines, daggers, magic sticks, cardboard fish, barrels of different colors, sticks, bows, swords, black hens, rosaries, saints, etc. were seized. They have all been locked up and will be prosecuted.
                    </p>

                    <h5>
                        *This research is an excerpt from my forthcoming book, Coding Witchcraft: Race, Religion, and Public Health. I would like to thank Laura Kurgan and Adeline Chum for inviting me to participate in the Center for Spatial Research workshops and for their feedback on this project. 
                    </h5>

                <h3 id="Tutorial">Technical Tutorial</h3>

                <h4 id="part1">Part 1:  Collecting data from newspapers</h4>
                <h4>Data</h4>
                    <p>
                        The data are retrieved from the Brazilian Digital Library can be accessed <a href="https://memoria.bn.gov.br/hdb/periodico.aspx">here</a>.
                    </p>
                    <p>The database includes a massive amount of articles broken down by newspaper and year of publication. This digital library has been particularly useful for our goals because of its ease of searching newspapers by individual terms. This has allowed us to quickly select specific articles in PDF format without having to search through entire newspapers. The digital library also provides certain metadata that we have integrated into our data collection process. In particular, we are able to gather the number of matches within a newspaper for terms over time.
                    </p>
                    <p>
                        An example of a search from the digital library homepage is shown below.
                    </p>

                    <div class="image">
                        <img src="./images/MappingReligiousPersecution/image2.png" alt="Search in Brazilian Digital Library">
                    </div>

                    <p>An example of the metadata for individual articles is found below.</p>
                    <div class="image">
                        <img src="./images/MappingReligiousPersecution/image3.png" alt="Search in Brazilian Digital Library">
                    </div>

                    <p>
                        The metadata were obtained from a mixed process of manually recording data and web scraping. The code that runs this automated process can be found on <a href="https://github.com/jared-lieberman/CSR-Tutorial/blob/main/scrape_metadata.py">the GitHub repository linked here</a>. 
                    </p>

                    <p>
                        The algorithm uses two files: 
                    </p>
                    <p class="indent-num">
                        1. a main script</p>
                    <p class="indent-next">
                        2. a configuration file
                    </p>
                    <p>
                        The configuration file (config.py) defines several constants used in the scraping process, including the search terms (<span class="code-block">SEARCH_TERMS</span>), the output file name(<span class="code-block">OUTPUT_FILE</span>), and a dictionary mapping newspaper IDs to their corresponding names (<span class="code-block">NEWSPAPER_ID_DICT</span>). Additionally, the script allows the user to specify which newspaper to search within by setting the <span  class="code-block">NEWSPAPER_ID</span>.
                    </p>

                    <p>
                        The main script (scrape_metadata.py) uses Selenium to automate interactions with the website. It begins by initializing a Chrome web driver and navigating to the appropriate URL based on the selected newspaper's ID. The script includes functions to handle pop-ups, extract data from the web page, and hover over elements to reveal additional options. It iterates through the specified search terms, performing searches on the website and extracting relevant data for each term. The extracted data is then compiled into a Pandas DataFrame, which is saved as a CSV file. The entire process is automated to run seamlessly, with informative print statements throughout to track the script's progress and any issues encountered.
                    </p>

                    <p>
                        In order to run the code, the user must have a Jupyter environment running and the required packages installed. Additionally, the user needs to define the four variables in the configuration file: 
                    </p>
                    <p class="indent-num">
                        1. <span class="code-block">SEARCH_TERMS</span> (list),</p>
                    <p class="indent-next">
                        2. <span class="code-block">OUTPUT_FILE</span> (CSV string name),</p>
                    <p class="indent-next">
                        3. <span class="code-block">NEWSPAPER_ID_DICT</span> (to add more newspapers to search), and </p>
                    <p class="indent-next">
                        4. <span class="code-block">NEWSPAPER_ID</span> (string found from dict).
                    </p>
                    <p>
                    
                        The code can search through one newspaper at a time. Once these are set, then code in Jupyter can be run.
                    </p>

                    <p>
                        From these article ID strings, in combination with ID’s for individual newspapers, we were able to recreate the URLs of each article. 
                    </p>
                    <p>
                        For example, the <span class="code-block">ID</span> for Gazeta de Notícias is <span class="code-block">103730</span>, and a full form URL of an article in that newspaper is “https://memoria.bn.gov.br/pdf/103730/per103730_1875_00052.pdf”. Thus, the article that has an ID of <span class="code-block">00052</span> and was published in 1875. This process was conducted in Python. Once we pieced together the URLs, research assistants were able to more efficiently access relevant articles. 
                    </p>

                    <p>
                        As noted, the metadata also allowed us to conduct temporal analysis on the number of articles that included the searched term in a particular newspaper. This process, conducted in R, reformatted the data in order to prepare it for making relevant visualizations. <a href="https://github.com/jared-lieberman/CSR-Tutorial/blob/main/count_years.Rmd">Click here for theGitHub repository code</a>. 
                    </p>  
                    <p>Below is a graph of the number of article matches over time by term and newspaper. For this graph, we include nine terms across two newspapers.
                    </p>

                    <div class="image">
                        <img src="./images/MappingReligiousPersecution/image4.png" alt="Search in Brazilian Digital Library">
                    </div>  


                <h4 id="part2">Part 2:  Text analysis, <i>More than a Map Visualization</i></h4>

                <p>
                    The second part of the analysis encompasses a combination of text analysis, geolocation, and mapping in order to geospatially visualize the language of witchcraft and its policing over time. From the text of each article, we are working on an algorithm to conduct natural language processing (NLP) to extract locations of the incidents. The algorithm used for extracting address-like entities is Named Entity Recognition (NER). NER is a sub-task of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. For our analysis, we are particularly interested in location-related entities from the articles. 
                </p>

                <p>
                    The Jupyter Notebook geolocate.ipynb is designed to geolocate addresses from the dataset of newspapers texts and append the corresponding latitude and longitude values to the data. 
                </p>
                <p>
                    Here's a breakdown of what the code does and how to run it:
                </p>

                <p>
                    The notebook starts by importing the necessary libraries: 
                </p>
                <p class="indent-num">
                    1. pandas for data manipulation </p>
                <p class="indent-next">
                    2. geopy for geocoding. </p>
                <p class="indent-next">
                    3. Nominatim, a geocoding service provided by OpenStreetMap, is used for converting addresses into geographical coordinates (latitude and longitude). </p>
                <p class="indent-next">
                    4. A RateLimiter from geopy.extra is also set up to manage the rate at which geocoding requests are sent, preventing the service from being overwhelmed and to comply with usage limits.
                </p>
                <p>
                    The code then loads a CSV file (witchcraft in print media - Sheet1.csv) into a Pandas DataFrame and removes any rows that have missing values in the 'Address' column. This ensures that only valid addresses are passed to the geocoding function.
                </p>

                <p>
                    A function named get_lat_long is defined to take an address as input and return the corresponding latitude and longitude. This function appends "Rio de Janeiro, Brazil" to the address to improve geocoding accuracy, assuming that all addresses are within this region. The geocoding is done using the Nominatim service. If an address is successfully geocoded, the latitude and longitude are returned; otherwise, None is returned for both.
                </p>

                <p>
                    The get_lat_long function is applied to the 'Address' column of the DataFrame, and the results are stored in new columns for latitude and longitude. The DataFrame is then printed to the console to display the results. Finally, the updated DataFrame, now including latitude and longitude data, is saved to a new CSV file (Output_geolocate.csv). 
                </p>
                <p>
                    In order to run the code, ensure that you have the required Python libraries installed. You can install them using pip if they are not already available in your environment. With the inputted CSV file in the same directory as the notebook, open the Jupyter Notebook and run each cell sequentially. The notebook will load the data, geocode the addresses, and output the results to both the console and a new CSV file (Output_geolocate.csv). After running the notebook, you can check the output CSV file to see the geocoded data, which now includes latitude and longitude for each address.
                </p>
                <p>
                    The next step is to fill in missing data manually by looking at the Address column for rows without latitude and longitude values produced by the automated process. Copy all or a subset of the cells, and append “Rio de Janeiro, Brazil” to the text. Then, using a mapping service like Google Maps, search for the address and grab the latitude and longitude values. 
                </p>

                <p>
                    Once those values are filled in, the next step is to map the data. The Jupyter Notebook (<a href="https://github.com/jared-lieberman/CSR-Tutorial/blob/main/Map_texts.ipynb">Map_texts.ipynb</a>) is designed to visualize geographic data by plotting points on a map using latitude and longitude coordinates. The notebook begins by splitting a column named lat_long_manual into two separate columns: latitude and longitude. The lat_long_manual column appears to store latitude and longitude as a single string, separated by a comma (e.g., "40.7128, -74.0060"). These new columns are then converted to numeric data types for proper handling in later steps.
                </p>

                <p>
                    The notebook uses the folium library to create an interactive map. The map is centered around the average of the provided latitude and longitude coordinates, ensuring that the map is centered on the general region where the data points are located. The zoom level is set to 2, which provides a global view of the map but can be adjusted depending on the density and proximity of the data points. For each row in the DataFrame, a marker is added to the map at the corresponding latitude and longitude. These markers represent the geographic locations specified in the dataset, allowing for a visual representation of the data on the map. The created map is saved to an HTML file (map.html), which can be opened in any web browser for interactive exploration. Additionally, the map can be displayed directly within the Jupyter Notebook by simply outputting the folium.Map object. 
                </p>
                <div class="image">
                    <img src="./images/MappingReligiousPersecution/image5.png" alt="Search in Brazilian Digital Library">
                </div>
                <p>
                    From here, we can start to <a href="https://github.com/jared-lieberman/CSR-Tutorial/blob/main/Map_texts_macumba.ipynb">adjust the map</a> to analyze geospatial patterns with language used within the articles. For example, below is a map where the red points include the term “macumba” or “macumbeiro”.
                </p>

                <div class="image">
                    <img src="./images/MappingReligiousPersecution/image6.png" alt="Search in Brazilian Digital Library">
                </div>


        
            </div>

            
                
        </div>
        <div id="project_tags" class="tag-grid"></div>
    </div>

        <script>
            let tutorialsData;
            let colorsData;

            function loadData() {
                // Load both JSON files
                d3.json("../tutorials.json").then(function(loadedData) {
                    tutorialsData = loadedData;
                    d3.json("../colors.json").then(function(loadedColors) {
                        colorsData = loadedColors;
                        displayTagsAndPanel();
                    });
                });
            }

            function getColorForTag(tag) {
                // Find the panel color for a given tag
                const colorEntry = colorsData.colors.find(colorEntry => colorEntry.tags.includes(tag));
                return colorEntry ? colorEntry.color : "#CCCCCC"; // Default to grey if not found
            }

            function getPanelColor(panel) {
                // Find the color for a given panel
                const colorEntry = colorsData.colors.find(colorEntry => colorEntry.panel === panel);
                return colorEntry ? colorEntry.color : "#CCCCCC"; // Default to grey if not found
            }

            function displayTagsAndPanel() {
                const projectName = document.body.getAttribute("data-project-name");
                const tagContainer = document.getElementById("project_tags");
                const panelTextElement = document.getElementById("panel-text");
                const circleElement = document.querySelector(".tutorials-circle");

                const project = tutorialsData.tutorials.find(tutorial => tutorial.project === projectName);

                if (project) {
                    panelTextElement.innerText = project.panel;

                    // Find and set the color for the panel
                    const panelColor = getPanelColor(project.panel);
                    circleElement.style.backgroundColor = panelColor;

                    // Display and color the tags
                    project.tags.forEach(tag => {
                        const tagElement = document.createElement("div");
                        tagElement.className = "tag-item";
                        tagElement.innerText = tag;
                        tagElement.style.backgroundColor = getColorForTag(tag); // Use color for each tag
                        tagContainer.appendChild(tagElement);
                    });
                }
            }

            document.getElementById('sidePanel').addEventListener('click', function() {
                window.location.href = '../index.html'; 
            });

            loadData(); // Call the loadData function to load the JSON and display the tags and panel text
        </script>

</body>
</html>
